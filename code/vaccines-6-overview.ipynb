{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99750a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from jupyter_client import find_connection_file\n",
    "connection_file = find_connection_file()\n",
    "print(connection_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1837a4d5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Plotting related\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.tracebacklimit = 0\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "# Scikit-learn related imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import randint, ttest_ind\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_recall_curve\n",
    "from sklearn.cluster import KMeans\n",
    "from math import log\n",
    "\n",
    "import importlib\n",
    "# Constants for this project\n",
    "import constants\n",
    "from constants import *\n",
    "\n",
    "\n",
    "import papermill as pm\n",
    "import shutil\n",
    "shutil.copy('constants.py', 'export/')\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(\"/home/yonatan/Documents/projects/vaccines/code\")\n",
    "\n",
    "\n",
    "sys.tracebacklimit = 0\n",
    "def exception_handler(exception_type, exception, traceback):\n",
    "    # All your trace are belong to us!\n",
    "    # your format\n",
    "    print(f\"{exception_type.__name__}, {exception}\")\n",
    "\n",
    "sys.excepthook = exception_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a4343",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define auxilary functions\n",
    "def get_dir_by_name(dir_name):\n",
    "    # Define the starting directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Traverse up the directory tree until we find a directory named \"data\"\n",
    "    while current_dir != \"/\":\n",
    "        if dir_name in os.listdir(current_dir):\n",
    "            data_dir = os.path.join(current_dir, dir_name)\n",
    "            return data_dir\n",
    "        current_dir = os.path.dirname(current_dir)\n",
    "    else:\n",
    "        print(f\"Directory {dir_name} not found in the parent directories.\")\n",
    "        raise (Exception())\n",
    "\n",
    "\n",
    "def remove_duplicate_accessions(dataset, immage_col, uid_col):\n",
    "    '''Sometimes there are multiple geo_accession numbers, like in GSE48018.SDY1276.\n",
    "    Average the IMMAGE, since all else is the same'''\n",
    "    first_uid = dataset.iloc[0][uid_col]\n",
    "    accessions = dataset[dataset[uid_col] == first_uid][\"geo_accession\"].unique()\n",
    "    if len(accessions) > 1:\n",
    "        # print(f\"Multiple accession detected, Collapsing by averaging on IMMAGE value\")\n",
    "        dataset = dataset.groupby(uid_col, as_index=False).agg({immage_col: \"mean\", **{col: \"first\" for col in dataset.columns if col not in [uid_col, immage_col]},})\n",
    "\n",
    "    accessions = dataset[dataset[uid_col] == first_uid][\"geo_accession\"].unique()\n",
    "    assert len(accessions) == 1\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_threshold_from_probability(prob, intercept, slope):\n",
    "    return -1 * (log(1 / prob - 1) + intercept) / slope\n",
    "\n",
    "def plot_response(data, dataset_name, strain, features=\"\"):\n",
    "    \"\"\"\n",
    "    Plots the response distribution based on specified features for a given dataset and strain.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the dataset to be plotted\n",
    "    - dataset_name: Name of the dataset\n",
    "    - strain: Strain information for the dataset\n",
    "    - features: List of features to be plotted (optional)\n",
    "\n",
    "    Returns:\n",
    "    - Displays a visual representation of the response distribution based on the specified features.\n",
    "\n",
    "    Example Usage:\n",
    "    plot_response(my_data, \"Example Dataset\", \"Strain A\", [\"Feature1\", \"Feature2\"])\n",
    "    \"\"\"\n",
    "    custom_palette = {\"Non-Responders\": \"orange\", \"Responders\": \"#3498db\"}\n",
    "    if len(features) == 1:\n",
    "        col_name = features[0]\n",
    "        # Plot sorted feature values vs Index on the second subplot\n",
    "        sorted_data = data.sort_values(col_name, ignore_index=True).reset_index()\n",
    "        sns.scatterplot(\n",
    "            data=sorted_data, x=\"index\", y=col_name, hue=\"Label text\", palette=custom_palette\n",
    "        )\n",
    "        \n",
    "        plt.set_title(f\"Sorted {col_name} vs Index\")\n",
    "\n",
    "    if len(features) == 2:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(8, 3))  # Creates a figure with two side-by-side subplots\n",
    "        col_name_1 = features[0]\n",
    "        col_name_2 = features[1]\n",
    "\n",
    "        sorted_data = data.sort_values(col_name_1, ignore_index=True).reset_index()\n",
    "        sns.scatterplot(\n",
    "            ax=axs[0], data=sorted_data, x=\"index\", y=col_name_1, hue=\"Label text\", palette=custom_palette\n",
    "        )\n",
    "        axs[0].set_title(f\"Sorted {col_name_1} vs Index\")\n",
    "\n",
    "        sorted_data = data.sort_values(col_name_2, ignore_index=True).reset_index()\n",
    "        sns.scatterplot(\n",
    "            ax=axs[1], data=sorted_data, x=\"index\", y=col_name_2, hue=\"Label text\", palette=custom_palette\n",
    "        )\n",
    "        axs[1].set_title(f\"Sorted {col_name_2} vs Index\")\n",
    "\n",
    "    fig.suptitle(f\"Response distribution: {dataset_name} {strain}\")\n",
    "    plt.tight_layout()  # Adjusts subplot params so that subplots fit into the figure area.\n",
    "    plt.show()\n",
    "\n",
    "def get_classifier_stats_prob(data, prob_column, prob_threshold):\n",
    "    # Global measures (entire dataset)\n",
    "    optimal_pred = data[prob_column].apply(lambda x: 1 if x >= prob_threshold else 0)\n",
    "    test_accuracy = accuracy_score(data[\"y\"], optimal_pred)\n",
    "    # Performance above the prob_threshold\n",
    "    y_over_thr = data.loc[data[prob_column] >= prob_threshold, [\"y\"]]\n",
    "    non_response_rate_over_thr = y_over_thr.mean().y\n",
    "    y_under_thr = data.loc[data[prob_column] < prob_threshold, [\"y\"]]\n",
    "    non_response_rate_under_thr = y_under_thr.mean().y\n",
    "    return non_response_rate_over_thr, non_response_rate_under_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset, P):\n",
    "    # These parameters are overridden by papermill\n",
    "    dataset_name = P[\"dataset_name\"]\n",
    "    bAdjustMFC = P[\"bAdjustMFC\"]\n",
    "    strain_index = P[\"strain_index\"]\n",
    "    day = P[\"day\"]\n",
    "    day0  = P[\"day0\"]\n",
    "    strains = P[\"strains\"]\n",
    "    strain = P[\"strain\"]\n",
    "    \n",
    "    if len(strains) > 1:\n",
    "        dataset = dataset.loc[dataset[strain_col] == strain].reset_index(drop=True)\n",
    "\n",
    "    # Discard seroprotected subjects based on HAI > 40 threshold)\n",
    "    if bDiscardSeroprotected:\n",
    "        day0_mask = dataset[day_col] == day0\n",
    "        threshold_mask = dataset[response_col]> HAI_threshold\n",
    "\n",
    "        # Get a list of all protected patients\n",
    "        serprotected_subjects = dataset.loc[(day0_mask) & (threshold_mask)][uid_col].unique()\n",
    "        # keep only patients not in the serprotected_subjects list\n",
    "        dataset = dataset.loc[~dataset[uid_col].isin(serprotected_subjects)]\n",
    "        subjects_left = dataset[uid_col].unique()\n",
    "        # print(f\"Discarding {len(serprotected_subjects)} seroprotected subjects\")\n",
    "        # print(f\"Subjects left: N={len(subjects_left)}\")\n",
    "\n",
    "    # Pivot the dataset such that different days' samples appear in their own columns, witn NaN where there are missing samples\n",
    "    t = dataset[[dataset_col, uid_col, age_col, immage_col,accesion_col , day_col, response_col]]\n",
    "    pivot_t = t.pivot_table(index=uid_col, columns=day_col, values=response_col, aggfunc='first')\n",
    "    age_t = dataset[['uid', 'Age']].drop_duplicates()\n",
    "\n",
    "    # Average IMMAGE values across geo_accessions (if they exist) and merge\n",
    "    immage_t = t.groupby('uid')[immage_col].mean()\n",
    "    tmp_t = age_t.merge(immage_t, on='uid', how='left').drop_duplicates()\n",
    "    pivot_t = tmp_t.merge(pivot_t, on='uid', how='left')\n",
    "\n",
    "    # Reset index to make uid a column again\n",
    "    pivot_t.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Remove the name of the columns and index name\n",
    "    pivot_t.columns.name = None\n",
    "    pivot_t.index.name = None\n",
    "\n",
    "    # Currently only used by AdjustMFC branch. TODO: convert the \"regular\" branch to use it too\n",
    "    pivot_dataset = pivot_t\n",
    "\n",
    "    # Use adjusted MFC (HAI) as per John Tsang\n",
    "    cluster_col = day0\n",
    "    if bAdjustMFC:\n",
    "        print(\"Preprocessing dataset, computing adjusted FC\")\n",
    "        metadata = pd.DataFrame(dataset_day_dicts_for_adjFC)\n",
    "        days = metadata[metadata[dataset_col] == dataset_name][\"Days\"].iloc[0]\n",
    "        sampleDay = [x for x in days if \"D0\" not in x][0]\n",
    "        day0 = [x for x in days if \"D0\" in x][0]\n",
    "        cluster_col = day0\n",
    "\n",
    "        # Pivot the table to have a column per day\n",
    "        dataset = pivot_dataset[[uid_col, immage_col, age_col, day0, sampleDay]]\n",
    "        dataset = dataset.loc[(~pivot_t[day0].isna()) & (~pivot_t[sampleDay].isna())] ;\n",
    "        dataset[\"FC\"] = dataset[sampleDay]/dataset[day0]\n",
    "\n",
    "        # Remove outliers\n",
    "        mean = dataset[day0].mean()\n",
    "        std = dataset[day0].std()\n",
    "        threshold = 3 * std\n",
    "        dataset = dataset[(dataset[day0] >= mean - threshold) & (dataset[day0] <= mean + threshold)]\n",
    "\n",
    "        # Bin subjects into 2-3 bins using k-means clustering\n",
    "        kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "        dataset['Cluster'] = kmeans.fit_predict(dataset[[cluster_col]])\n",
    "\n",
    "        def normalize(x):\n",
    "            return (x - x.median()) / x.std()\n",
    "\n",
    "        # Normalize the FC within each bin to obtain the adjFC\n",
    "        dataset['adjFC'] = dataset.groupby('Cluster')[\"FC\"].transform(normalize)\n",
    "\n",
    "        # Take relevant columns only\n",
    "        data = dataset[[immage_col, 'adjFC', age_col, cluster_col, \"Cluster\"]].rename(columns={'adjFC': response_col}).dropna()\n",
    "        # data.groupby(\"Cluster\").count()\n",
    "\n",
    "    else: # bAdjustMFC == False\n",
    "        # If not computing adjMFC, take a specific strain from the given post-vaccine day & assay\n",
    "        day_mask = dataset[day_col] == day\n",
    "        dataset = dataset.loc[(day_mask)].reset_index(drop=True)\n",
    "\n",
    "        dataset = remove_duplicate_accessions(dataset, immage_col, uid_col)\n",
    "\n",
    "        # Take relevant columns only\n",
    "        data = dataset[[immage_col, response_col, age_col]]\n",
    "\n",
    "    # Keep older subjects only, since that's what's actually interesting, and may show IMMAGE's advantage\n",
    "    if bOlderOnly == True:\n",
    "        young_subjects = data.loc[data[age_col] < age_threshlod]\n",
    "        data = data.loc[data[age_col] >= age_threshlod]\n",
    "        if len(data) == 0:\n",
    "            raise(Exception(\"No subjects over the age of {age_threshlod}. Exiting.\"))\n",
    "        # print(f\"Discarding {len(young_subjects)} seroprotected subjects\")\n",
    "        # print(f\"Subjects left: N={len(data)}\")\n",
    "\n",
    "\n",
    "    #### Dataset & Strain info\n",
    "    age_restrict_str = f\", Subjects over the age of {age_threshlod}\" if bOlderOnly else \"\"\n",
    "    day_str = \"Adjusted MFC\" if bAdjustMFC else f\"day: {day}\"\n",
    "\n",
    "    # print(f\"\"\"### Analysis for dataset: {dataset_name}, strain: {strain}, {day_str}{age_restrict_str}\"\"\")\n",
    "\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Get a boolean map of sub and super threshold values\n",
    "    low_response_thr = data[[response_col]].quantile(q=0.3).item()\n",
    "\n",
    "    # Generate labels\n",
    "    # Note that we define y=1 for all responses <= 30th percentile (and not <)\n",
    "    # Also note that we defined y=1 as *non* responders, since later on that's what we'll care about detecting\n",
    "    data[\"y\"] = data[response_col].apply(lambda x: 1 if x <= low_response_thr else 0)\n",
    "\n",
    "    # Add a text label for plot legends\n",
    "    data[\"Label text\"] = data[\"y\"].apply(lambda x: \"Responders\" if x == 0 else \"Non-Responders\")\n",
    "\n",
    "\n",
    "    plot_response(data, dataset_name, strain, features=[immage_col, age_col])\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_datasets(datasets, metadata):\n",
    "    accumulated_results = pd.DataFrame()\n",
    "    for dataset_name in metadata[dataset_col].unique():\n",
    "            curr_metadata = metadata.loc[metadata[dataset_col] == dataset_name]\n",
    "            dataset = datasets.loc[datasets[dataset_col] == dataset_name]\n",
    "            print(dataset_name)\n",
    "            day = [x for x in curr_metadata[\"Days\"].iloc[0] if \"D0\" not in x][0]\n",
    "            strains = dataset.loc[dataset[day_col] == day][strain_col].unique()\n",
    "            if len(strains) < 1:\n",
    "                 strains = [\"single strain - data missing\"]\n",
    "            if len(strains) > 1:\n",
    "                strains = list(set(strains) - set([\"Influenza\"])) # denotes an MFC calculation in the original dataset\n",
    "            # print(strains)\n",
    "            for strain_index in range(len(strains)):\n",
    "                strain_name = strains[strain_index].replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "                # print(f'exporting {dataset_name}, strain no. {strain_index}: {strain_name}, day: {day}')\n",
    "                # Define parameters for curr_metadata and strain\n",
    "                day0 = [x for x in curr_metadata[\"Days\"].iloc[0] if \"D0\" in x][0]\n",
    "                P = {\n",
    "                    \"bAdjustMFC\": bAdjustMFC,\n",
    "                    \"dataset_name\": dataset_name,\n",
    "                    \"strain_index\": strain_index,\n",
    "                    \"strain\": strains[strain_index],\n",
    "                    \"strains\": strains,\n",
    "                    \"day\":  day,\n",
    "                    \"day0\":  day0,\n",
    "                }\n",
    "                try:\n",
    "                    tmp_dict = {\n",
    "                        dataset_col: dataset_name,\n",
    "                        strain_col: strain_name,\n",
    "                        strain_index_col: strain_index,\n",
    "                        day_col: \"AdjMFC\" if bAdjustMFC else f\"{day}\",\n",
    "                        \"bAdjustMFC\" :  bAdjustMFC,\n",
    "                    }\n",
    "                    analyze_dataset(dataset, P)\n",
    "                except Exception as e:\n",
    "                    print (f\"Caught exception when runnnig {dataset_name}\")\n",
    "                    raise(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def main():\n",
    "\n",
    "# Read in Data and drop missing values\n",
    "data_dir = get_dir_by_name(\"data\")\n",
    "df = pd.read_csv(os.path.join(data_dir, \"../data/all_vaccines.csv\"))\n",
    "datasets = df.dropna(subset=[immage_col, age_col, dataset_col, uid_col, day_col, response_col])\n",
    "dataset_names = datasets[dataset_col].unique()\n",
    "# Get the info for all influenza datasets, excluding some.\n",
    "influenza_df = pd.DataFrame(influenza_dicts)\n",
    "all_sets_df = pd.DataFrame(dataset_day_dicts_for_adjFC)\n",
    "\n",
    "# We can choose to analyze all datasets, only influenza ones and only non-influenza ones\n",
    "if bInfluenza:\n",
    "    # Get the info for all influenza datasets, excluding some.\n",
    "    metadata = influenza_df\n",
    "    dataset_names = metadata[dataset_col].unique().astype(str)\n",
    "    dataset_names = list(set(dataset_names) - set(exclude_datasets))\n",
    "elif bNonInfluenza:\n",
    "    metadata = all_sets_df\n",
    "    dataset_names = all_sets_df[dataset_col].unique().astype(str)\n",
    "    dataset_names = list(set(dataset_names) - set(influenza_df[dataset_col]) - set(exclude_datasets))\n",
    "else:\n",
    "    metadata = all_sets_df\n",
    "    dataset_names = all_sets_df[dataset_col].unique().astype(str)\n",
    "    dataset_names = list(set(dataset_names) - set(exclude_datasets))\n",
    "\n",
    "# Filter datasets and metadata according to the list of datasets we want to look at.\n",
    "datasets = datasets.loc[datasets[\"Dataset\"].isin(dataset_names)]\n",
    "metadata =  metadata.loc[metadata[\"Dataset\"].isin(dataset_names)]\n",
    "\n",
    "age_restrict_str = f\", subjects over the age of {age_threshlod}\" if bOlderOnly else \"\"\n",
    "print(f\"\"\"Analysis parameters: discarding serprotected subjects: {bDiscardSeroprotected}{age_restrict_str}\"\"\")\n",
    "\n",
    "# Turn on debug here for running a single dataset through analyze_dataset()\n",
    "bDebug = False\n",
    "if bDebug:\n",
    "    # Narrow to a specific datset\n",
    "    dataset_name = \"GSE125921.SDY1529\"\n",
    "    # Filter data\n",
    "    name_mask = datasets[dataset_col] == dataset_name\n",
    "    dataset = datasets.loc[name_mask].reset_index(drop=True)\n",
    "\n",
    "    # Filter metadata\n",
    "    name_mask = metadata[dataset_col] == dataset_name\n",
    "    metadata = metadata.loc[name_mask].reset_index(drop=True)\n",
    "\n",
    "    day = [x for x in metadata[\"Days\"].iloc[0] if \"D0\" not in x][0]\n",
    "    day0 = [x for x in metadata[\"Days\"].iloc[0] if \"D0\" in x][0]\n",
    "    \n",
    "    strains = dataset.loc[dataset[day_col] == day][strain_col].unique()\n",
    "    print(strains)\n",
    "    if len(strains) > 1:\n",
    "        strains = list(set(strains) - set([\"Influenza\"])).sort() # denotes an MFC calculation in the original dataset\n",
    "    if len(strains) < 1:\n",
    "        strains = ['single strain - missing data']\n",
    "    \n",
    "    strain_index = 0\n",
    "    P = {\n",
    "        \"bAdjustMFC\": bAdjustMFC,\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"strain_index\": strain_index,\n",
    "        \"day\":  day,\n",
    "        \"day0\":  day0,\n",
    "        \"strain\": strains[strain_index],\n",
    "        \"strains\": strains\n",
    "    }\n",
    "    analyze_dataset(dataset, P)\n",
    "else:\n",
    "    analyze_all_datasets(datasets, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28362790",
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3887e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaccines",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}