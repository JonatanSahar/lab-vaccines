{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca9c8bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from jupyter_client import find_connection_file\n",
    "connection_file = find_connection_file()\n",
    "print(connection_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316be3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting related\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.tracebacklimit = 0\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "# Scikit-learn related imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import randint, ttest_ind\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_recall_curve\n",
    "from sklearn.cluster import KMeans\n",
    "from math import log\n",
    "\n",
    "import importlib\n",
    "# Constants for this project\n",
    "import constants\n",
    "from constants import *\n",
    "\n",
    "\n",
    "import papermill as pm\n",
    "import shutil\n",
    "shutil.copy('constants.py', 'export/')\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(\"/home/yonatan/Documents/projects/vaccines/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17acf716",
   "metadata": {},
   "outputs": [],
   "source": [
    "bAdjustMFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353013a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define auxilary functions\n",
    "def get_dir_by_name(dir_name):\n",
    "    # Define the starting directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Traverse up the directory tree until we find a directory named \"data\"\n",
    "    while current_dir != \"/\":\n",
    "        if dir_name in os.listdir(current_dir):\n",
    "            data_dir = os.path.join(current_dir, dir_name)\n",
    "            return data_dir\n",
    "        current_dir = os.path.dirname(current_dir)\n",
    "    else:\n",
    "        print(f\"Directory {dir_name} not found in the parent directories.\")\n",
    "        raise (Exception())\n",
    "\n",
    "\n",
    "def remove_duplicate_accessions(dataset, immage_col, uid_col):\n",
    "    '''Sometimes there are multiple geo_accession numbers, like in GSE48018.SDY1276.\n",
    "    Average the IMMAGE, since all else is the same'''\n",
    "    first_uid = dataset.iloc[0][uid_col]\n",
    "    accessions = dataset[dataset[uid_col] == first_uid][\"geo_accession\"].unique()\n",
    "    if len(accessions) > 1:\n",
    "        # print(f\"Multiple accession detected, Collapsing by averaging on IMMAGE value\")\n",
    "        dataset = dataset.groupby(uid_col, as_index=False).agg({immage_col: \"mean\", **{col: \"first\" for col in dataset.columns if col not in [uid_col, immage_col]},})\n",
    "\n",
    "    accessions = dataset[dataset[uid_col] == first_uid][\"geo_accession\"].unique()\n",
    "    assert len(accessions) == 1\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_threshold_from_probability(prob, intercept, slope):\n",
    "    return -1 * (log(1 / prob - 1) + intercept) / slope\n",
    "\n",
    "def calc_and_plot_prob_threshold(data, classifier, precision, recall, thresholds, prob_column, features=\"\"):\n",
    "    AUC = auc(recall, precision)\n",
    "    intercept = classifier.intercept_[0]\n",
    "    slope = classifier.coef_[0][0]\n",
    "\n",
    "    naive_classification_precision = data[\"y\"].mean()\n",
    "\n",
    "    # Identifying the optimal threshold (maximal F1 score)\n",
    "    beta = 0.7\n",
    "    F_scores = (1+pow(beta, 2))*(precision * recall)/(pow(beta, 2)*precision + recall)\n",
    "    optimal_idx = np.nanargmax(F_scores)\n",
    "    prob_threshold = thresholds[optimal_idx]\n",
    "    score = F_scores[optimal_idx]\n",
    "\n",
    "    # Calculate the cutoff value\n",
    "    feature_threshold = get_threshold_from_probability(\n",
    "        prob_threshold, intercept=intercept, slope=slope\n",
    "    )\n",
    "\n",
    "    return (score, prob_threshold, feature_threshold, AUC)\n",
    "\n",
    "\n",
    "def get_classifier_stats_prob(data, prob_column, prob_threshold):\n",
    "    # Global measures (entire dataset)\n",
    "    optimal_pred = data[prob_column].apply(lambda x: 1 if x >= prob_threshold else 0)\n",
    "    test_accuracy = accuracy_score(data[\"y\"], optimal_pred)\n",
    "    # Performance above the prob_threshold\n",
    "    y_over_thr = data.loc[data[prob_column] >= prob_threshold, [\"y\"]]\n",
    "    non_response_rate_over_thr = y_over_thr.mean().y\n",
    "    y_under_thr = data.loc[data[prob_column] < prob_threshold, [\"y\"]]\n",
    "    non_response_rate_under_thr = y_under_thr.mean().y\n",
    "    return non_response_rate_over_thr, non_response_rate_under_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b36d1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset, P):\n",
    "    # These parameters are overridden by papermill\n",
    "    dataset_name = dataset[dataset_col].iloc[0]\n",
    "    bAdjustMFC = P[\"bAdjustMFC\"]\n",
    "    strain_index = P[\"strain_index\"]\n",
    "    day = P[\"day\"]\n",
    "    day0  = P[\"day0\"]\n",
    "    dayMFC = P[\"dayMFC\"]\n",
    "\n",
    "    # Discard seroprotected subjects based on HAI > 40 threshold)\n",
    "    if bDiscardSeroprotected:\n",
    "        day0_mask = dataset[day_col] == day0\n",
    "        threshold_mask = dataset[response_col]> HAI_threshold\n",
    "\n",
    "        # Get a list of all protected patients\n",
    "        serprotected_subjects = dataset.loc[(day0_mask) & (threshold_mask)][uid_col].unique()\n",
    "        # keep only patients not in the serprotected_subjects list\n",
    "        dataset = dataset.loc[~dataset[uid_col].isin(serprotected_subjects)]\n",
    "        subjects_left = dataset[uid_col].unique()\n",
    "        print(f\"Discarding {len(serprotected_subjects)} seroprotected subjects\")\n",
    "        print(f\"Subjects left: N={len(subjects_left)}\")\n",
    "\n",
    "    # Pivot the dataset such that different days' samples appear in their own columns, witn NaN where there are missing samples\n",
    "    t = dataset[[dataset_col, uid_col, age_col, immage_col,accesion_col , day_col, response_col]]\n",
    "    pivot_t = t.pivot_table(index=uid_col, columns=day_col, values=response_col, aggfunc='first')\n",
    "    age_t = dataset[['uid', 'Age']].drop_duplicates()\n",
    "\n",
    "    # Average IMMAGE values across geo_accessions (if they exist) and merge\n",
    "    immage_t = t.groupby('uid')[immage_col].mean()\n",
    "    tmp_t = age_t.merge(immage_t, on='uid', how='left').drop_duplicates()\n",
    "    pivot_t = tmp_t.merge(pivot_t, on='uid', how='left')\n",
    "\n",
    "    # Reset index to make uid a column again\n",
    "    pivot_t.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Remove the name of the columns and index name\n",
    "    pivot_t.columns.name = None\n",
    "    pivot_t.index.name = None\n",
    "\n",
    "    # TODO complete generating MFC for this dataset\n",
    "    # if dataset_name == 'SDY296' or dataset_name == 'GSE48023.SDY1276':\n",
    "    #     dataset = dataset.loc[(~pivot_t[day0].isna()) & (~pivot_t[\"FC.HAI\"].isna())]\n",
    "    #     pivot_t[dayMFC] = pivot_t[day] / pivot_t[day0]\n",
    "\n",
    "    # Currently only used by AdjustMFC branch. TODO: convert the \"regular\" branch to use it too\n",
    "    pivot_dataset = pivot_t\n",
    "\n",
    "    # Use adjusted MFC (HAI) as per John Tsang\n",
    "    cluster_col = day0\n",
    "    if bAdjustMFC:\n",
    "        # raise(Exception(\"Did you mean to use adjMFC?\"))\n",
    "        dataset = pivot_dataset[[uid_col, immage_col, age_col, day0, dayMFC]]\n",
    "        print(\"Preprocessing dataset, computing adjusted MFC (HAI)\")\n",
    "        dataset = dataset.loc[(~pivot_t[day0].isna()) & (~pivot_t[dayMFC].isna())]\n",
    "\n",
    "        mean = dataset[day0].mean()\n",
    "        std = dataset[day0].std()\n",
    "        threshold = 3 * std\n",
    "        dataset = dataset[(dataset[day0] >= mean - threshold) & (dataset[day0] <= mean + threshold)]\n",
    "\n",
    "        # Bin subjects into 2-3 bins using k-means clustering\n",
    "        kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "        dataset['Cluster'] = kmeans.fit_predict(dataset[[cluster_col]])\n",
    "\n",
    "        def normalize(x):\n",
    "            return (x - x.median()) / x.std()\n",
    "\n",
    "        # Normalize the MFC within each bin to obtain the adjMFC\n",
    "        dataset['adjMFC'] = dataset.groupby('Cluster')[dayMFC].transform(normalize)\n",
    "\n",
    "        # Take relevant columns only\n",
    "        data = dataset[[immage_col, 'adjMFC', age_col, cluster_col, \"Cluster\"]].rename(columns={'adjMFC': response_col}).dropna()\n",
    "        # data.groupby(\"Cluster\").count()\n",
    "        strain = \"Influenza\"\n",
    "        strains = \"Influenza\"\n",
    "\n",
    "    else: # bAdjustMFC == False\n",
    "        # If not computing adjMFC, take a specific strain from the given post-vaccine day & assay\n",
    "        dayMFC_mask = dataset[day_col] == day\n",
    "        dataset = dataset.loc[(dayMFC_mask)].reset_index(drop=True)\n",
    "\n",
    "        # Somtimes there are multiple strains - so multiple rows per day\n",
    "        strains = dataset[strain_col].unique()\n",
    "        if len(strains) > 1:\n",
    "            dataset = dataset.loc[dataset[strain_col] == strains[strain_index]].reset_index(drop=True)\n",
    "\n",
    "        strains_t = dataset[strain_col].unique()\n",
    "        assert len(strains_t) == 1\n",
    "        strain = strains_t[0]\n",
    "\n",
    "        dataset = remove_duplicate_accessions(dataset, immage_col, uid_col)\n",
    "\n",
    "        # Take relevant columns only\n",
    "        data = dataset[[immage_col, response_col, age_col]]\n",
    "\n",
    "    # Keep older subjects only, since that's what's actually more interesting, and may show IMMAGE's advantage\n",
    "    if bOlderOnly == True:\n",
    "        young_subjects = data.loc[data[age_col] < age_threshlod]\n",
    "        data = data.loc[data[age_col] >= age_threshlod]\n",
    "        if len(data) == 0:\n",
    "            raise(Exception(\"No subjects over the age of {age_threshlod}. Exiting.\"))\n",
    "        print(f\"Discarding {len(young_subjects)} seroprotected subjects\")\n",
    "        print(f\"Subjects left: N={len(data)}\")\n",
    "\n",
    "\n",
    "    #### Dataset & Strain info\n",
    "    age_restrict_str = f\", Subjects over the age of {age_threshlod}\" if bOlderOnly else \"\"\n",
    "    day_str = \"Adjusted MFC\" if bAdjustMFC else f\"day: {day}\"\n",
    "\n",
    "    print(f\"\"\"### Analysis for dataset: {dataset_name}, strain: {strain}, {day_str}{age_restrict_str}\"\"\")\n",
    "\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Get a boolean map of sub and super threshold values\n",
    "    low_response_thr = data[[response_col]].quantile(q=0.3).item()\n",
    "\n",
    "    # Generate labels\n",
    "    # Note that we define y=1 for all responses <= 30th percentile (and not <)\n",
    "    # Also note that we defined y=1 as *non* responders, since later on that's what we'll care about detecting\n",
    "    data[\"y\"] = data[response_col].apply(lambda x: 1 if x <= low_response_thr else 0)\n",
    "\n",
    "    # Add a text label for plot legends\n",
    "    data[\"Label text\"] = data[\"y\"].apply(lambda x: \"Responders\" if x == 0 else \"Non-Responders\")\n",
    "\n",
    "\n",
    "    # Classifying with logistic regression - fit on the entire dataset\n",
    "    log_regress_immage = LogisticRegression()\n",
    "    log_regress_age = LogisticRegression()\n",
    "    log_regress_combined = LogisticRegression()\n",
    "\n",
    "    # Train a classifier based on immage and on age for comparison\n",
    "    log_regress_immage.fit(data[[immage_col]], data[\"y\"])\n",
    "    log_regress_age.fit(data[[age_col]], data[\"y\"])\n",
    "    log_regress_combined.fit(data[[immage_col, age_col]], data[\"y\"])\n",
    "\n",
    "    non_responder_col = \"p_non_responder\"\n",
    "    non_responder_col_age = \"p_non_responder_age\"\n",
    "    non_responder_col_combined = \"p_non_responder_combined\"\n",
    "\n",
    "    proba = pd.DataFrame(log_regress_immage.predict_proba(data[[immage_col]]))\n",
    "    data[non_responder_col] = proba[1]\n",
    "    proba = pd.DataFrame(log_regress_age.predict_proba(data[[age_col]]))\n",
    "    data[non_responder_col_age] = proba[1]\n",
    "    proba = pd.DataFrame(log_regress_combined.predict_proba(data[[immage_col, age_col]]))\n",
    "    data[non_responder_col_combined] = proba[1]\n",
    "\n",
    "    # #### Thresholding based on logistic regression probabilties\n",
    "    # #### IMMAGE-based classification\n",
    "    # Run for immage and age to compare\n",
    "    # IMMAGE\n",
    "    precision, recall, thresholds = precision_recall_curve(data[\"y\"], data[non_responder_col])\n",
    "    immage_score, prob_threshold, immage_threshold, immage_auc = calc_and_plot_prob_threshold(\n",
    "        data, log_regress_immage, precision, recall, thresholds, non_responder_col, features=[immage_col]\n",
    "    )\n",
    "    non_response_rate_over_thr, non_response_rate_under_thr = get_classifier_stats_prob(\n",
    "        data, non_responder_col, prob_threshold\n",
    "    )\n",
    "\n",
    "\n",
    "    # #### Age-based classification\n",
    "    # Age\n",
    "    precision, recall, thresholds = precision_recall_curve(data[\"y\"], data[non_responder_col_age])\n",
    "    age_score, prob_threshold_age, age_threshold, age_auc = calc_and_plot_prob_threshold(\n",
    "        data, log_regress_age, precision, recall, thresholds, non_responder_col_age, features=[age_col]\n",
    "    )\n",
    "    age_non_response_rate_over_thr, age_non_response_rate_under_thr = get_classifier_stats_prob(\n",
    "        data, non_responder_col_age, prob_threshold_age\n",
    "    )\n",
    "\n",
    "\n",
    "    # #### Age & IMMAGE combined\n",
    "    # Combined\n",
    "    precision, recall, thresholds = precision_recall_curve(data[\"y\"], data[non_responder_col_combined])\n",
    "    combined_score, prob_threshold_combined, _, combined_auc = calc_and_plot_prob_threshold(\n",
    "        data, log_regress_combined, precision, recall, thresholds, non_responder_col_combined, features=[immage_col, age_col]\n",
    "    )\n",
    "    combined_non_response_rate_over_thr, combined_non_response_rate_under_thr = (\n",
    "        get_classifier_stats_prob(data, non_responder_col_combined, prob_threshold_combined)\n",
    "    )\n",
    "\n",
    "\n",
    "    # #### Comparison of using the different features\n",
    "    summary_dict = {\n",
    "    (\"F score\", \"IMMAGE\"): [immage_score],\n",
    "    (\"F score\", \"Age\"): [age_score],\n",
    "    (\"F score\", \"Multivariate\"): [combined_score],\n",
    "    (\"NR rate over threshold\", \"IMMAGE\"): [non_response_rate_over_thr],\n",
    "    (\"NR rate over threshold\", \"Age\"): [age_non_response_rate_over_thr],\n",
    "    (\"NR rate over threshold\", \"Multivariate\"): [combined_non_response_rate_over_thr],\n",
    "    (\"NR rate under threshold\", \"IMMAGE\"): [non_response_rate_under_thr],\n",
    "    (\"NR rate under threshold\", \"Age\"): [age_non_response_rate_under_thr],\n",
    "    (\"NR rate under threshold\", \"Multivariate\"): [combined_non_response_rate_under_thr],\n",
    "}\n",
    "\n",
    "    # Create a MultiIndex\n",
    "    multi_index = pd.MultiIndex.from_product([[\"F score\", \"NR rate over threshold\", \"NR rate under threshold\"], [\"IMMAGE\", \"Age\", \"Multivariate\"]])\n",
    "\n",
    "    # Create the DataFrame\n",
    "    summary = pd.DataFrame(summary_dict, columns=multi_index)\n",
    "    summary[\"Composite\", \"IMMAGE\"] = summary[[(\"F score\", \"IMMAGE\"), (\"NR rate over threshold\", \"IMMAGE\")]].mean(axis=1)\n",
    "    summary[\"Composite\", \"Age\"] = summary[[(\"F score\", \"Age\"), (\"NR rate over threshold\", \"Age\")]].mean(axis=1)\n",
    "    summary[\"Composite\", \"Multivariate\"] = summary[[(\"F score\", \"Multivariate\"), (\"NR rate over threshold\", \"Multivariate\")]].mean(axis=1)\n",
    "    # print(summary.to_string(index=False))\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_datasets(datasets, metadata):\n",
    "    accumulated_results = pd.DataFrame()\n",
    "    for bAdjustMFC in [True, False]:\n",
    "        for dataset_name in metadata[dataset_col].unique():\n",
    "                curr_metadata = metadata.loc[metadata[dataset_col] == dataset_name]\n",
    "                dataset = datasets.loc[datasets[dataset_col] == dataset_name]\n",
    "                print(dataset_name)\n",
    "                days = curr_metadata[\"Days\"].iloc[0]\n",
    "                for day in days:\n",
    "                        adjMFC_str = \"AdjMFC\" if bAdjustMFC else f\"day: {day}\"\n",
    "                        strains = dataset.loc[dataset[day_col] == day][strain_col].unique()\n",
    "                        # print(strains)\n",
    "                        for strain_index in range(len(strains)):\n",
    "                                strain_name = strains[strain_index].replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "                                # print(f'exporting {dataset_name}, strain no. {strain_index}: {strain_name}, day: {day}')\n",
    "                                # Define parameters for curr_metadata and strain\n",
    "                                P = {\n",
    "                                    \"bAdjustMFC\": bAdjustMFC,\n",
    "                                    \"dataset_name\": dataset_name,\n",
    "                                    \"strain_index\": strain_index,\n",
    "                                    \"day\":  day,\n",
    "                                    \"day0\":  curr_metadata[\"Day0\"].iloc[0],\n",
    "                                    \"dayMFC\":  curr_metadata[\"DayMFC\"].iloc[0],\n",
    "                                }\n",
    "                                try:\n",
    "                                    tmp_dict = {\n",
    "                                        dataset_col: dataset_name,\n",
    "                                        strain_col: strain_name,\n",
    "                                        strain_index_col: strain_index,\n",
    "                                        day_col: \"AdjMFC\" if bAdjustMFC else f\"{day}\",\n",
    "                                        \"bAdjustMFC\" :  bAdjustMFC,\n",
    "                                    }\n",
    "                                    # initialize the row for this database with some metadata\n",
    "                                    row  = pd.DataFrame([tmp_dict])\n",
    "                                    row = pd.concat([row, analyze_dataset(dataset, P)], axis=1) # Concat along lines, adding new columns\n",
    "                                    accumulated_results = pd.concat([accumulated_results, row], ignore_index=True) # Concat along columns, adding new lines\n",
    "                                except:\n",
    "                                        print (f\"******\\nCaught exception when runnnig {dataset_name}\\n******\\n\")\n",
    "    return accumulated_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "\n",
    "# Read in Data and drop missing values\n",
    "data_dir = get_dir_by_name(\"data\")\n",
    "df = pd.read_csv(os.path.join(data_dir, \"../data/all_vaccines.csv\"))\n",
    "datasets = df.dropna(subset=[immage_col, age_col, dataset_col, uid_col, day_col, response_col])\n",
    "dataset_names = datasets[dataset_col].unique()\n",
    "\n",
    "if bInfluenza:\n",
    "    # Get the info for all influenza datasets, excluding some.\n",
    "    metadata = pd.DataFrame(influenza_dicts)\n",
    "    dataset_names = metadata[dataset_col].unique().astype(str)\n",
    "    dataset_names = list(set(dataset_names) - set(exclude_datasets))\n",
    "    datasets = datasets.loc[datasets[\"Dataset\"].isin(dataset_names)]\n",
    "    print(\"Working with Influenza datasets only\")\n",
    "\n",
    "\n",
    "# # Narrow to a specific datset\n",
    "# dataset_name = \"GSE41080.SDY212\"\n",
    "# # Filter data\n",
    "# name_mask = datasets[dataset_col] == dataset_name\n",
    "# dataset = datasets.loc[name_mask].reset_index(drop=True)\n",
    "\n",
    "# # Filter metadata\n",
    "# name_mask = metadata[dataset_col] == dataset_name\n",
    "# metadata = metadata.loc[name_mask].reset_index(drop=True)\n",
    "\n",
    "# P = {\n",
    "#     \"dataset_name\": dataset_name,\n",
    "#     \"strain_index\": 0,\n",
    "#     \"day\":  metadata[\"Days\"].iloc[0][0],\n",
    "#     \"day0\":  metadata[\"Day0\"].iloc[0],\n",
    "#     \"dayMFC\":  metadata[\"DayMFC\"].iloc[0],\n",
    "# }\n",
    "\n",
    "# This will print a sort of log of the different analyses\n",
    "results = analyze_all_datasets(datasets, metadata)\n",
    "\n",
    "# Get all the analyses that look somewhat promising based on the composite (F1 and over-threshold rate) metric\n",
    "score_mask = (results[\"Composite\", \"IMMAGE\"] >  results[\"Composite\", \"Age\"]) | (results[\"Composite\", \"Multivariate\"] >  results[\"Composite\", \"Age\"])\n",
    "cols_to_access = [\n",
    "    dataset_col,\n",
    "    strain_col,\n",
    "    strain_index_col,\n",
    "    day_col,\n",
    "]\n",
    "\n",
    "# Dynamically add all sub-columns for 'Composite'\n",
    "composite_columns = [col for col in results.columns if col[0] == 'Composite']\n",
    "cols_to_access.extend(composite_columns)\n",
    "results = results.loc[score_mask, cols_to_access] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0aa3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate figures for promising datasets\n",
    "for index, row in results.iterrows():\n",
    "    parameters = {\n",
    "        \"bAdjustMFC\" : row[\"bAdjustMFC\"],\n",
    "        \"dataset_name\": row[dataset_col],\n",
    "        \"strain_index\": row[strain_index_col],\n",
    "        \"day\": row[day_col]\n",
    "    }\n",
    "    print(f'exporting {row[dataset_col]}, strain no. {row[strain_index_col]}: {row[strain_col]}, day: {row[day_col]}')\n",
    "    output_notebook_name = f\"{row[dataset_col]}_{row[strain_col]}_{row[day_col]}{seroprotected_str}{age_restrict_str}\"\n",
    "    output_notebook = f\"export/{output_notebook_name}.ipynb\"\n",
    "    try:\n",
    "            pm.execute_notebook(\n",
    "                    input_path=\"vaccines-4.ipynb\",\n",
    "                    output_path=output_notebook,\n",
    "                    parameters=parameters,\n",
    "                    prepare_only=True\n",
    "            )\n",
    "    except Exception as e:\n",
    "            print (f\"******\\nCaught exception when runnnig {output_notebook}\\n******\\n\")\n",
    "            raise(e)\n",
    "    # Export the executed notebook to HTML\n",
    "    output_html = f\"{output_notebook_name}.html\"\n",
    "    os.system(f\"jupyter nbconvert --execute --no-input --to html {output_notebook} --output {output_html}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ebb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save promising results to a spreadsheet\n",
    "def generate_html_path(row):\n",
    "    output_html = f\"{row[dataset_col]}_{row[strain_col]}_{row[day_col]}{seroprotected_str}{age_restrict_str}.html\"\n",
    "    output_html = os.path.join(get_dir_by_name('code'), \"export\", output_html)    \n",
    "    output_html = '=HYPERLINK(\"file://' + output_html + '\", \"Link\")'\n",
    "    return output_html\n",
    "\n",
    "results = results.loc[score_mask, cols_to_access] \n",
    "results['html_path'] = results.apply(generate_html_path, axis=1)\n",
    "results.to_csv(f\"export/results_{seroprotected_str}{age_restrict_str}.csv\", index=False, float_format='%.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaccines",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}